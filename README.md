# ğŸ  HousePrices ML Pipeline

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-blue?style=flat-square&logo=python&logoColor=white" alt="Python"/>
  <img src="https://img.shields.io/badge/scikit--learn-1.7.2-orange?style=flat-square&logo=scikit-learn&logoColor=white" alt="scikit-learn"/>
  <img src="https://img.shields.io/badge/RÂ²-0.9394-brightgreen?style=flat-square" alt="RÂ² Score"/>
  <img src="https://img.shields.io/badge/Kaggle-0.13049-success?style=flat-square" alt="Kaggle RMSLE"/>
  <img src="https://img.shields.io/badge/License-MIT-yellow?style=flat-square" alt="License"/>
</p>

<p align="center">
  <strong>Production-ready modular ML pipeline - End-to-end workflow for regression tasks</strong><br>
  <em>Data â†’ Feature Engineering â†’ Preprocessing â†’ Training â†’ Evaluation â†’ Reporting</em>
</p>

<p align="center">
  <a href="#-quick-start">Quick Start</a> â€¢
  <a href="#-features">Features</a> â€¢
  <a href="#-architecture">Architecture</a> â€¢
  <a href="#-performance">Performance</a> â€¢
  <a href="#-usage">Usage</a>
</p>

---

## ğŸš€ Quick Start

```bash
# 1. Clone repository
git clone https://github.com/4F71/HousePrices-ML-Pipeline.git
cd HousePrices-ML-Pipeline

# 2. Setup environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt

# 3. Run pipeline
python -m scripts.train_eval --model ridge
```

**That's it!** The pipeline automatically:
- Loads data and applies feature engineering
- Trains RidgeCV model with 5-fold cross-validation
- Saves model to `models/houseprice.joblib`
- Writes metrics to `reports/metrics.json`
- Generates feature importance plot at `figures/importance.png`

---

## ğŸ“Š Performance

| Metric | Score | Description |
|--------|-------|-------------|
| **RÂ² (Local)** | **0.9394** | Model explains 93.9% of price variance |
| **RMSE (Local)** | **0.0984** | 9.8% average deviation in log-transformed prices |
| **Kaggle RMSLE** | **0.13049** | Validated score on Kaggle competition |

**Analysis:**  
The model achieves strong performance with minimal feature engineering. The validated Kaggle score of **0.13049 RMSLE** demonstrates robust generalization to unseen data (top 40% leaderboard).

<p align="center">
  <img src="figures/importance.png" alt="Feature Importance" width="700"/>
</p>

### ğŸ† Kaggle Validation
This pipeline achieved **0.13049 RMSLE** on  
[Kaggle â€“ House Prices: Advanced Regression Techniques](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)

---

## âœ¨ Features

### ğŸ§  **Feature Engineering**
- **`TotalSF`** â†’ Total living area (1stFlrSF + 2ndFlrSF + TotalBsmtSF)
- **`BathCount`** â†’ Total bathrooms (FullBath + 0.5Ã—HalfBath)
- **`Age`** â†’ House age (2020 - YearBuilt)
- **Log transformation** â†’ Applied to `GrLivArea`, `TotalSF`, `SalePrice`
- **Outlier filtering** â†’ Removes extreme `GrLivArea` values

### âš™ï¸ **Preprocessing Pipeline**
- **Numeric features:**
  - Missing values â†’ SimpleImputer (mean strategy)
  - Scaling â†’ StandardScaler
- **Categorical features:**
  - Missing values â†’ SimpleImputer (most frequent strategy)
  - Encoding â†’ OneHotEncoder (handles unknown categories)

### ğŸ¤– **Model Selection**
4 regression models with cross-validation support:

```bash
python -m scripts.train_eval --model linear    # LinearRegression
python -m scripts.train_eval --model ridge     # RidgeCV (L2, default)
python -m scripts.train_eval --model lasso     # LassoCV (L1)
python -m scripts.train_eval --model elastic   # ElasticNetCV (L1+L2)
```

**Default (RidgeCV):**
- Alpha values: [0.1, 1.0, 10.0]
- Cross-validation: 5-fold
- L2 regularization prevents overfitting

### ğŸ“ˆ **Automated Reporting**
- **JSON metrics:** `reports/metrics.json`
- **Feature importance:** `figures/importance.png`
- **Model persistence:** `models/houseprice.joblib`

---

## ğŸ—‚ï¸ Architecture

```
HousePrices/
â”‚
â”œâ”€â”€ data/                    # Raw datasets
â”‚   â”œâ”€â”€ train.csv
â”‚   â””â”€â”€ test.csv
â”‚
â”œâ”€â”€ models/                  # Trained models
â”‚   â””â”€â”€ houseprice.joblib
â”‚
â”œâ”€â”€ reports/                 # Performance metrics
â”‚   â””â”€â”€ metrics.json
â”‚
â”œâ”€â”€ figures/                 # Visualizations
â”‚   â””â”€â”€ importance.png
â”‚
â”œâ”€â”€ src/                     # Core modules
â”‚   â”œâ”€â”€ paths.py            # Project path management
â”‚   â”œâ”€â”€ data.py             # Data loading utilities
â”‚   â”œâ”€â”€ features.py         # Feature engineering
â”‚   â”œâ”€â”€ preprocess.py       # Preprocessing pipeline
â”‚   â”œâ”€â”€ model.py            # Model factory
â”‚   â”œâ”€â”€ pipeline.py         # Pipeline builder
â”‚   â”œâ”€â”€ train.py            # Training logic
â”‚   â”œâ”€â”€ eval.py             # Evaluation metrics
â”‚   â””â”€â”€ visualize.py        # Feature importance plots
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train_eval.py       # End-to-end execution script
â”‚
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ”„ Pipeline Flow

```mermaid
graph LR
    A[data/train.csv] --> B[feature_engineer]
    B --> C[build_preprocessor]
    C --> D[model training]
    D --> E[evaluation]
    E --> F[reports/metrics.json]
    E --> G[figures/importance.png]
    D --> H[models/houseprice.joblib]
```

**Step-by-step:**
1. **Data loading** â†’ `src/data.py`
2. **Feature engineering** â†’ `src/features.py`
3. **Preprocessor creation** â†’ `src/preprocess.py` (numeric + categorical transformers)
4. **Pipeline creation** â†’ `src/pipeline.py` (preprocessor + model)
5. **Training & saving** â†’ `src/train.py` (80/20 split, random_state=42)
6. **Evaluation** â†’ `src/eval.py` (RÂ², RMSE)
7. **Visualization** â†’ `src/visualize.py` (top 15 features)

---

## ğŸ“– Usage

### Basic Training
```bash
python -m scripts.train_eval --model ridge
```

### Modular Usage
```python
# Data loading and feature engineering
from src.data import load_data
from src.features import feature_engineer

df = load_data("train.csv")
df = feature_engineer(df)

# Preprocessor creation
from src.preprocess import build_preprocessor

numeric = df.select_dtypes(include=["int32", "int64", "float32", "float64"]).columns.tolist()
categorical = df.select_dtypes(include=["object"]).columns.tolist()
numeric.remove("SalePrice")

preprocessor = build_preprocessor(numeric, categorical)

# Model training
from src.model import get_model
from src.train import train_and_save

model = get_model("ridge")
train_and_save(df, preprocessor, model)
```

### Generate Feature Importance Plot
```bash
python src/visualize.py
```

### Load Saved Model
```python
import joblib

model = joblib.load("models/houseprice.joblib")
predictions = model.predict(X_test)
```

---

## ğŸ› ï¸ Tech Stack

| Category | Technology |
|----------|-----------|
| **Language** | Python 3.10+ |
| **ML Framework** | scikit-learn 1.7.2 |
| **Data Processing** | pandas 2.3.3, numpy 2.2.6 |
| **Visualization** | matplotlib 3.10.7, seaborn 0.13.2 |
| **Serialization** | joblib 1.5.2 |
| **Notebook** | JupyterLab 4.4.10 (optional) |

---

## ğŸ“‚ Dataset

This project uses the **Kaggle House Prices: Advanced Regression Techniques** dataset.

ğŸ”— [Download Dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)

**Structure:**
- `train.csv` â†’ 1460 samples, 81 features
- `test.csv` â†’ 1459 samples (for submission)

Place both files in the `data/` directory before running the pipeline.

---

## ğŸ¯ Design Principles

âœ… **Modularity** â†’ Each component has a single responsibility  
âœ… **Reproducibility** â†’ Fixed random seeds, version-locked dependencies  
âœ… **PEP 257 Compliance** â†’ Docstrings for all public functions  
âœ… **Production-Ready** â†’ Clean separation of concerns, no hardcoded paths  
âœ… **Extensibility** â†’ Easy to add new models or feature engineering steps

---

## ğŸ¤ Contributing

This is a portfolio project demonstrating ML engineering best practices. You can:
- Fork and experiment
- Suggest improvements via issues
- Use as a template for your own projects

---

## ğŸ“œ License

**MIT License** Â© 2025 Onur Tilki

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software.

**THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND.**

---

## ğŸ‘¨â€ğŸ’» Author

**Onur Tilki**

- ğŸŒ GitHub: [@4F71](https://github.com/4F71)
- ğŸ“Š Kaggle: [@onurtilki](https://www.kaggle.com/onurtilki)
- ğŸ’¼ LinkedIn: [onurtilki](https://www.linkedin.com/in/onurtilki/)

---

## ğŸ™ Acknowledgments

- Dataset: [Kaggle House Prices Competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)
- Inspiration: Production-level ML systems and software engineering best practices

---

<p align="center">
  <strong>â­ If you find this useful, please consider starring the repository!</strong>
</p>

